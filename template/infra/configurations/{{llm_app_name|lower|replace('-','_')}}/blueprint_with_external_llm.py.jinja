# Copyright 2025 DataRobot, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""
This configuration option is right choice when you already have an LLM from Azure, Bedrock,
Anthropic, Vertex, etc. This way you can monitor and scale your LLM directly with the added
benefits of the DataRobot platform such as governance, guard models, controlled API access,
and monitoring.
"""

import datarobot as dr
import pulumi
import pulumi_datarobot as datarobot
from datarobot_pulumi_utils.pulumi import export
from datarobot_pulumi_utils.pulumi.stack import PROJECT_NAME
from datarobot_pulumi_utils.pulumi.custom_model_deployment import (
    CustomModelDeployment,
    DeploymentArgs,
    RegisteredModelArgs,
)
from datarobot_pulumi_utils.schema.custom_models import CustomModelArgs
from datarobot_pulumi_utils.schema.llms import (
    LLMSettings,
    LLMBlueprintArgs,
)
from datarobot_pulumi_utils.schema.exec_envs import RuntimeEnvironments

from . import use_case
from .lib{{ llm_app_name|lower|replace('-','_')}} import (
    get_runtime_values,
    validate_feature_flags,
    verify_llm,
)

__all__ = [
    "custom_model_runtime_parameters",
    "app_runtime_parameters",
    "default_model",
    "{{llm_app_name}}_application_name",
    "{{llm_app_name}}_resource_name",
]

REQUIRED_FEATURE_FLAGS = {
    "ENABLE_MLOPS": True,
    "ENABLE_CUSTOM_INFERENCE_MODEL": True,
    "ENABLE_PUBLIC_NETWORK_ACCESS_FOR_ALL_CUSTOM_MODELS": True,
    "ENABLE_MLOPS_TEXT_GENERATION_TARGET_TYPE": True,
}

__all__ = [
    "{{llm_app_name}}_application_name",
    "{{llm_app_name}}_resource_name",
]

{{llm_app_name}}_application_name: str = "{{llm_app_name}}"
{{llm_app_name}}_resource_name: str = "[{{llm_app_name}}]"
default_model: str = "azure-openai-gpt-4-o"

validate_feature_flags(REQUIRED_FEATURE_FLAGS)
llm_credential_runtime_params = get_runtime_values(default_model)
# This will ensure your credentials are working properly
# https://docs.litellm.ai/docs/providers for more details
# on what string to pass to `verify_llm` This default
# example is assuming Azure OpenAI with a OPENAI_API_DEPLOYMENT_ID='gpt-4o'.
# You combine that with azure/gpt-4o for LiteLLM to verify the model.
# Similar instructions exist for Bedrock: https://docs.litellm.ai/docs/providers/bedrock
# and Vertex: https://docs.litellm.ai/docs/providers/vertex
verify_llm("azure/gpt-4o")

playground = datarobot.Playground(
    use_case_id=use_case.id,
    resource_name="LLM Playground " + {{llm_app_name}}_resource_name,
)

llm_blueprint_args = LLMBlueprintArgs(
    resource_name="LLM Blueprint " + {{llm_app_name}}_resource_name,
    llm_id=default_model,
    llm_settings=LLMSettings(
        max_completion_length=2048,
        temperature=0.1,
        top_p=None,
    ),
)

llm_blueprint = datarobot.LlmBlueprint(
    playground_id=playground.id,
    **llm_blueprint_args.model_dump(),
)

custom_model_args = CustomModelArgs(
    resource_name="LLM Custom Model " + {{llm_app_name}}_resource_name,
    name="LLM Custom Model " + {{llm_app_name}}_resource_name,
    target_name="resultText",
    target_type=dr.enums.TARGET_TYPE.TEXT_GENERATION,
    replicas=1,
    base_environment_id=RuntimeEnvironments.PYTHON_312_MODERATIONS.value.id,
)

llm_custom_model = datarobot.CustomModel(
    **custom_model_args.model_dump(exclude_none=True),
    use_case_ids=[use_case.id],
    source_llm_blueprint_id=llm_blueprint.id,
    runtime_parameter_values=llm_credential_runtime_params,
)

registered_model_args = RegisteredModelArgs(
    resource_name="LLM Registered Model " + {{llm_app_name}}_resource_name,
)

prediction_environment = datarobot.PredictionEnvironment(
    resource_name="LLM Prediction Environment " + {{llm_app_name}}_resource_name,
    platform=dr.enums.PredictionEnvironmentPlatform.DATAROBOT_SERVERLESS,
)

deployment_args = DeploymentArgs(
    resource_name="LLM Deployment Args " + {{llm_app_name}}_resource_name,
    label=f"LLM Deployment [{PROJECT_NAME}] " + {{llm_app_name}}_resource_name,
    association_id_settings=datarobot.DeploymentAssociationIdSettingsArgs(
        column_names=["association_id"],
        auto_generate_id=False,
        required_in_prediction_requests=True,
    ),
    predictions_data_collection_settings=datarobot.DeploymentPredictionsDataCollectionSettingsArgs(
        enabled=True,
    ),
    predictions_settings=(
        datarobot.DeploymentPredictionsSettingsArgs(min_computes=0, max_computes=2)
    ),
)

llm_deployment = CustomModelDeployment(
    resource_name="LLM Deployment " + {{llm_app_name}}_resource_name,
    use_case_ids=[use_case.id],
    custom_model_version_id=llm_custom_model.version_id,
    registered_model_args=registered_model_args,
    prediction_environment=prediction_environment,
    deployment_args=deployment_args,
)

app_runtime_parameters = [
    datarobot.ApplicationSourceRuntimeParameterValueArgs(
        key={{llm_app_name}}_application_name.upper() + "_DEPLOYMENT_ID",
        type="string",
        value=llm_deployment.id,
    ),
    datarobot.ApplicationSourceRuntimeParameterValueArgs(
        key={{llm_app_name}}_application_name.upper() + "_DEFAULT_MODEL",
        type="string",
        value=default_model,
    ),
    datarobot.ApplicationSourceRuntimeParameterValueArgs(
        key={{llm_app_name}}_application_name.upper() + "_DEFAULT_MODEL_FRIENDLY_NAME",
        type="string",
        value=proxy_llm_registered_model.name,
    ),
]
custom_model_runtime_parameters = [
    datarobot.CustomModelRuntimeParameterValueArgs(
        key="LLM_DEPLOYMENT_ID",
        type="string",
        value=llm_deployment.id,
    ),
    datarobot.CustomModelRuntimeParameterValueArgs(
        key={{llm_app_name}}_application_name.upper() + "_DEFAULT_MODEL",
        type="string",
        value=default_model,
    ),
]
pulumi.export("Deployment ID " + {{llm_app_name}}_resource_name, llm_deployment.id)
export("LLM_DEPLOYMENT_ID", llm_deployment.id)
export("{{llm_app_name|upper}}_DEFAULT_MODEL", default_model)
export("{{llm_app_name|upper}}_DEFAULT_MODEL_FRIENDLY_NAME", llm_deployment.name)
