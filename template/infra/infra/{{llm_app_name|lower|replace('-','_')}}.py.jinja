# Copyright 2025 DataRobot, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""
This module is the "frontend" for the selected LLM module for other infra modules
to import resources and attributes.
i.e., `from .llm import app_runtime_parameters, default_llm`
will pull in those attributes from the active LLM module.
"""

from dataclasses import dataclass, field
from enum import Enum
import importlib
import os
import json
from pathlib import Path
import re
import sys
from types import ModuleType
from typing import Any
from contextlib import contextmanager

import datarobot
import pulumi
import pulumi_datarobot
from litellm import completion


from datarobot_pulumi_utils.common.feature_flags import (
    eval_feature_flag_statuses,
    FeatureFlagSet,
)

INFRA_DIR = Path(__file__).parent
TEMPLATES = {
    "{{llm_app_name|lower|replace('-','_')}}__deployed_llm",
    "{{llm_app_name|lower|replace('-','_')}}__llm_blueprint_with_llm_gateway",
    "{{llm_app_name|lower|replace('-','_')}}__llm_blueprint_with_provided_llm",
    "{{llm_app_name|lower|replace('-','_')}}__llm_gateway_direct",
}

# Cache for the active module to avoid repeated imports
_active_module_cache: ModuleType | None = None


class CredentialType(Enum):
    """Types of credentials supported by DataRobot."""
    API_TOKEN = "api_token"
    AWS = "aws"
    GOOGLE_CLOUD = "google_cloud"
    STRING = "string"


@dataclass
class CredentialMapping:
    """Maps LiteLLM environment variable names to DataRobot parameter names."""
    litellm_name: str
    datarobot_name: str
    credential_type: CredentialType
    is_secret: bool = True


@dataclass
class LLMCredentials:
    """Loads and manages LLM credentials for all available providers."""
    # Core environment variables that LiteLLM uses
    LITELLM_ENV_VARS = [
        "OPENAI_API_KEY", "OPENAI_API_BASE", "OPENAI_API_VERSION", "OPENAI_ORGANIZATION",
        "AZURE_API_KEY", "AZURE_API_BASE", "AZURE_API_VERSION", "OPENAI_API_DEPLOYMENT_ID",
        "AWS_ACCESS_KEY_ID", "AWS_SECRET_ACCESS_KEY", "AWS_SESSION_TOKEN", "AWS_REGION",
        "GOOGLE_SERVICE_ACCOUNT", "GOOGLE_REGION", "GOOGLE_APPLICATION_CREDENTIALS",
        "VERTEX_PROJECT", "VERTEX_LOCATION", "GOOGLE_PROJECT_ID",
    ]
    # Map variables that actually need transformation between LiteLLM and DataRobot
    CREDENTIAL_TRANSFORMATIONS = {
        "azure": [
            # Azure uses AZURE_* but DataRobot expects OPENAI_* for Azure OpenAI
            CredentialMapping("AZURE_API_KEY", "OPENAI_API_KEY", CredentialType.API_TOKEN),
            CredentialMapping("AZURE_API_BASE", "OPENAI_API_BASE", CredentialType.STRING, False),
            CredentialMapping("AZURE_API_VERSION", "OPENAI_API_VERSION", CredentialType.STRING, False),
        ],

        "vertex_ai": [
            # Vertex AI credential name variations
            CredentialMapping("VERTEX_PROJECT", "GOOGLE_PROJECT_ID", CredentialType.STRING, False),
            CredentialMapping("VERTEX_LOCATION", "GOOGLE_REGION", CredentialType.STRING, False),
        ],
    }
    credentials: dict[str, str] = field(default_factory=dict, init=False)
    available_providers: set[str] = field(default_factory=set, init=False)

    def __post_init__(self):
        """Load all available LLM environment variables on initialization."""
        self.credentials = {}
        self.available_providers = set()

        for env_var in LLMCredentials.LITELLM_ENV_VARS:
            value = os.environ.get(env_var)
            if value:
                self.credentials[env_var] = value

        # Determine which providers are available based on credentials
        if any(key.startswith("AZURE_") for key in self.credentials):
            self.available_providers.add("azure")
            self.available_providers.add("openai")
        if any(key.startswith("VERTEX_") for key in self.credentials) or any(key.startswith("GOOGLE_") for key in self.credentials):
            self.available_providers.add("vertex_ai")
        if any(key.startswith("AWS_") for key in self.credentials):
            self.available_providers.add("aws")
        if any(key.startswith("OPENAI_") for key in self.credentials):
            self.available_providers.add("openai")
            self.available_providers.add("azure")

    def transform(self) -> dict[str, str]:
        """Convert to DataRobot parameter format with transformations applied for all providers."""
        transformed = {}
        transformed_vars = set()

        # Apply transformations for all available providers
        for provider in self.available_providers:
            mappings = LLMCredentials.CREDENTIAL_TRANSFORMATIONS.get(provider, [])

            for mapping in mappings:
                if mapping.litellm_name in self.credentials:
                    # Add the transformed DataRobot variable name
                    transformed[mapping.datarobot_name] = self.credentials[mapping.litellm_name]
                    transformed_vars.add(mapping.litellm_name)

                    # Also add the original LiteLLM variable name unless the DataRobot name is already set
                    if mapping.datarobot_name not in self.credentials:
                        transformed[mapping.litellm_name] = self.credentials[mapping.litellm_name]

        # Pass through all other variables as-is (ones that don't need transformation)
        for key, value in self.credentials.items():
            if key not in transformed_vars:
                transformed[key] = value

        return transformed

    @contextmanager
    def as_environment_variables(self, use_datarobot_format: bool = True):
        """
        Context manager that temporarily sets environment variables for all providers.

        Args:
            use_datarobot_format: If True, applies DataRobot transformations.
                                If False, uses original LiteLLM format.

        Usage:
            with credentials.as_environment_variables():
                # Environment variables are set with DataRobot format for all providers
                result = some_operation_that_needs_env_vars()
            # Environment variables are restored to original state
        """
        if use_datarobot_format:
            env_vars = self.transform()
        else:
            env_vars = self.credentials

        # Store original environment state
        original_env = {}
        keys_to_restore = set()

        for key, value in env_vars.items():
            if key in os.environ:
                original_env[key] = os.environ[key]
            else:
                keys_to_restore.add(key)
            os.environ[key] = value

        try:
            yield env_vars
        finally:
            # Restore original environment
            for key in env_vars:
                if key in original_env:
                    os.environ[key] = original_env[key]
                elif key in keys_to_restore:
                    os.environ.pop(key, None)

    def create_datarobot_runtime_parameters(
        self,
        app_name: str,
    ) -> list[pulumi_datarobot.CustomModelRuntimeParameterValueArgs]:
        """
        Create DataRobot credential objects from all available credentials.
        Handles credentials for all loaded providers.
        """
        credentials = self.transform()

        if not credentials:
            raise pulumi.error(f"No credentials found")

        runtime_params = []
        stack_name = pulumi.get_stack()

        # Handle different credential types based on what we have
        for cred_key, cred_value in credentials.items():
            if not cred_value:
                continue

            param_key = cred_key

            # Create appropriate DataRobot credential type
            if cred_key in ["AWS_ACCESS_KEY_ID", "AWS_SECRET_ACCESS_KEY"]:
                # Group AWS credentials together
                if (
                    cred_key == "AWS_ACCESS_KEY_ID"
                    and "AWS_SECRET_ACCESS_KEY" in credentials
                ):
                    aws_credential = pulumi_datarobot.AwsCredential(
                        resource_name=f"{app_name} AWS Credential [{stack_name}]",
                        aws_access_key_id=credentials["AWS_ACCESS_KEY_ID"],
                        aws_secret_access_key=credentials["AWS_SECRET_ACCESS_KEY"],
                        aws_session_token=credentials.get("AWS_SESSION_TOKEN"),
                    )
                    runtime_params.append(
                        pulumi_datarobot.CustomModelRuntimeParameterValueArgs(
                            key="AWS_ACCOUNT",
                            type="credential",
                            value=aws_credential.id,
                        )
                    )
                # Skip individual AWS keys since we handle them as a group
                continue

            elif cred_key == "GOOGLE_APPLICATION_CREDENTIALS":
                # Handle Google credentials
                try:
                    with open(cred_value, "r") as f:
                        service_account_key = json.load(f)

                    google_credential = pulumi_datarobot.GoogleCloudCredential(
                        resource_name=f"{app_name} Google Credential [{stack_name}]",
                        gcp_key=json.dumps(service_account_key),
                    )
                    runtime_params.append(
                        pulumi_datarobot.CustomModelRuntimeParameterValueArgs(
                            key="GOOGLE_SERVICE_ACCOUNT",
                            type="credential",
                            value=google_credential.id,
                        )
                    )
                except Exception as e:
                    pulumi.warn(f"Failed to load Google credentials from {cred_value}: {e}")
                continue

            else:
                # Handle API key credentials
                if "key" in cred_key.lower() or "token" in cred_key.lower():
                    api_credential = pulumi_datarobot.ApiTokenCredential(
                        resource_name=f"{app_name} {cred_key} [{stack_name}]",
                        api_token=cred_value,
                    )
                    runtime_params.append(
                        pulumi_datarobot.CustomModelRuntimeParameterValueArgs(
                            key=param_key,
                            type="credential",
                            value=api_credential.id,
                        )
                    )
                else:
                    # Handle string parameters (endpoints, versions, etc.)
                    runtime_params.append(
                        pulumi_datarobot.CustomModelRuntimeParameterValueArgs(
                            key=param_key,
                            type="string",
                            value=cred_value,
                        )
                    )

        return runtime_params


def _get_active_module() -> ModuleType | None:
    """Get the active LLM module, loading and caching it if necessary."""
    global _active_module_cache

    if _active_module_cache is not None:
        return _active_module_cache

    # Find the active module file
    active_file = None
    for llm_module in INFRA_DIR.glob("{{llm_app_name|lower|replace('-','_')}}__*.py"):
        if llm_module.is_file():
            active_file = llm_module
            break

    if active_file is None:
        pulumi.warn("No LLM is selected for {{llm_app_name}} to choose from")
        return None

    # Import and cache the active module using absolute import
    module_stem = active_file.stem
    # Get 'infra.infra' from 'infra.infra.{{llm_app_name|lower|replace('-','_')}}'
    current_package = __name__.rsplit(".", 1)[0]
    module_name = f"{current_package}.{module_stem}"

    try:
        _active_module_cache = importlib.import_module(module_name)
        return _active_module_cache
    except ImportError as e:
        raise ImportError(f"Failed to import active LLM module {module_name}: {e}")


def __getattr__(name: str) -> Any:
    """
    Lazy attribute lookup that delegates to the active LLM module.

    This gets called when an attribute is not found in the current module,
    allowing us to dynamically proxy attributes from the active LLM module.
    """
    # Avoid intercepting Python special attributes to prevent recursion
    if name.startswith("__") and name.endswith("__"):
        raise AttributeError(f"module '{__name__}' has no attribute '{name}'")

    try:
        active_module = _get_active_module()
        if hasattr(active_module, name):
            # Cache the attribute in our module for faster subsequent access
            value = getattr(active_module, name)
            setattr(sys.modules[__name__], name, value)
            return value
    except Exception as e:
        pulumi.warn(f"Failed to get attribute '{name}' from active LLM module: {e}")

    # If we can't find it, raise the standard AttributeError
    raise AttributeError(f"module '{__name__}' has no attribute '{name}'")


def validate_feature_flags(flags: FeatureFlagSet) -> None:
    corrections, invalid_flags = eval_feature_flag_statuses(flags)
    for flag in invalid_flags:
        correct_value = flags[flag]
        pulumi.warn(
            f"Feature flag '{flag}' is required to be {correct_value} but is no longer a valid DataRobot feature flag."
        )
    if invalid_flags:
        raise ValueError(f"Invalid feature flags detected: {invalid_flags}")
    for flag, correct_value in corrections:
        pulumi.error(
            f"This app template requires that feature flag '{flag}' is set "
            f"to {correct_value}. Contact your DataRobot representative for "
            "assistance."
        )
    if corrections:
        raise pulumi.RunError("Please correct feature flag settings and run again.")


def verify_llm_gateway_model_availability(model_id: str) -> None:
    """
    Validate the model is in the catalog
    """
    dr_client = datarobot.Client()
    response = dr_client.get("genai/llmgw/catalog/")
    data = response.json()
    non_deprecated_models = "\n.   - ".join(
        [
            model["model"]
            for model in data["data"]
            if not model["isDeprecated"] and model["isActive"]
        ]
    )
    matched_models = [
        model
        for model in data["data"]
        if (model["model"] == model_id or model["llmId"] == model_id)
    ]
    if not matched_models:
        raise ValueError(
            f"Model '{model_id}' not found in catalog. Available models: {non_deprecated_models}"
        )
    if not len(matched_models) == 1:
        raise ValueError(
            f"Multiple models found for '{model_id}' in catalog. {matched_models}"
        )
    if not matched_models[0]["isActive"] or matched_models[0]["isDeprecated"]:
        raise ValueError(
            f"Model '{model_id}' is not active or is deprecated. Available models: {non_deprecated_models}"
        )


def verify_llm(model_id: str | None, deployment_id: str | None = None) -> None:
    """
    Verify that the specified LLM is valid, available, and you can say hello
    """

    # Pre-existing deployment
    if deployment_id:
        dr_client = datarobot.Client()
        deployment_chat_base_url = (
            f"{dr_client.endpoint.rstrip('/')}/deployments/{deployment_id}/"
        )
        completion(
            model="datarobot/datarobot-deployed-llm",
            messages=[{"content": "Hi", "role": "user"}],
            api_base=deployment_chat_base_url,
        )
        return

    if model_id is None:
        raise ValueError("model_id must be provided to verify_llm")
    # Remove after https://github.com/BerriAI/litellm/pull/13880
    # is released
    completion_extra_kwargs = dict()
    base_url = None
    if model_id.startswith("datarobot/"):
        dr_client = datarobot.Client()
        base_url = re.sub(r"api/v2/?$", "", dr_client.endpoint)
        completion_extra_kwargs["base_url"] = base_url
    completion(
        model=model_id,
        messages=[{"content": "Hi", "role": "user"}],
        **completion_extra_kwargs,
    )


def validate_single_selection() -> None:
    """Validate that exactly one LLM module exists."""
    enabled_llms = []
    for llm_module in INFRA_DIR.glob("{{llm_app_name|lower|replace('-','_')}}__*.py"):
        if llm_module.is_file():
            enabled_llms.append(llm_module.name)
    if not enabled_llms:
        pulumi.warn("No LLM modules are enabled for {{llm_app_name}}.")

    if len(enabled_llms) != 1:
        found_llms = "\n  - ".join(enabled_llms)
        disable_llm_config = "\n".join(
            [f"INFRA_ENABLE_{llm.upper()[:-3]}=false" for llm in enabled_llms]
        )
        raise ValueError(
            f"Exactly one LLM module must be enabled for {{llm_app_name}}. Select one by renaming the LLM modules you don't want with a `.inactive` suffix or using environment variables to disable them with `INFRA_ENABLE_<filename>=false`.\nFound: \n  - {found_llms}\n\nTo disable them with environment variables:\n{disable_llm_config}"
        )


def init():
    """Code that needs to run on import."""
    validate_single_selection()


init()
