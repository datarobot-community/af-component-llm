# Copyright 2025 DataRobot, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""
This module is useful when you already have an LLM Deployed.
It will pull it into the playground and use case
"""
import os
from datarobot_pulumi_utils.pulumi import export
from datarobot_pulumi_utils.pulumi.proxy_llm_blueprint import (
    ProxyLLMBlueprint,
    LLMBlueprintArgs,

)
import pulumi
import pulumi_datarobot as datarobot

from . import use_case
from .__{{ llm_app_name|lower|replace('-','_')}}__ import (
    validate_feature_flags, verify_llm, verify_llm_gateway_model_availability
)

REQUIRED_FEATURE_FLAGS = {
    "ENABLE_MLOPS": True,
    "ENABLE_CUSTOM_INFERENCE_MODEL": True,
    "ENABLE_PUBLIC_NETWORK_ACCESS_FOR_ALL_CUSTOM_MODELS": True,
    "ENABLE_MLOPS_TEXT_GENERATION_TARGET_TYPE": True,
}

TEXTGEN_DEPLOYMENT_ID = os.environ.get("TEXTGEN_DEPLOYMENT_ID")

{{llm_app_name}}_application_name: str = "{{llm_app_name}}"
{{llm_app_name}}_resource_name: str = "[{{llm_app_name}}]"
default_llm: str = "my-llm"

# Verify everything is working
validate_feature_flags(REQUIRED_FEATURE_FLAGS)
verify_llm(deployment_id=TEXTGEN_DEPLOYMENT_ID)

playground = datarobot.Playground(
    use_case_id=use_case.id,
    resource_name="LLM Playground " + {{llm_app_name}}_resource_name,
)
proxy_llm_deployment = datarobot.Deployment.get(
    resource_name="Existing LLM Deployment", id=TEXTGEN_DEPLOYMENT_ID
)
llm_blueprint_args = LLMBlueprintArgs(
    resource_name="LLM Blueprint " + {{llm_app_name}}_resource_name,
    llm_id=default_llm,
    llm_settings=LLMSettings(
        max_completion_length=2048,
        temperature=0.1,
        top_p=None,
    ),
)
llm_blueprint = ProxyLLMBlueprint(
    use_case_id=use_case.id,
    playground_id=playground.id,
    proxy_llm_deployment_id=proxy_llm_deployment.id,
    chat_model_name=default_llm,
)

app_runtime_parameters = [
    datarobot.ApplicationSourceRuntimeParameterValueArgs(
        key={{llm_app_name}}_application_name.upper() + "_DEPLOYMENT_ID",
        type="string",
        value=proxy_llm_blueprint.id,
    ),
    datarobot.ApplicationSourceRuntimeParameterValueArgs(
        key={{llm_app_name}}_application_name.upper() + "_MODEL_ID",
        type="string",
        value=default_llm,
    ),
]
custom_model_runtime_parameters = [
    datarobot.CustomModelRuntimeParameterValueArgs(
        key="LLM_DEPLOYMENT_ID",
        type="string",
        value=llm_deployment.id,
    )
]

pulumi.export("Deployment ID " + {{llm_app_name}}_resource_name, proxy_llm_blueprint.id)
export("LLM_DEPLOYMENT_ID", proxy_llm_blueprint.id)