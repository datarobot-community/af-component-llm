# Copyright 2025 DataRobot, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""
This module is a control module that toggles various LLM deployment templates on the fly, and validates
that there is only one selected at a time per copy of the template
"""

import os
import json
from pathlib import Path
from typing import Any, Dict, List, Optional
import datarobot
import pulumi
import pulumi_datarobot
from litellm import completion
from litellm.utils import get_llm_provider

from datarobot_pulumi_utils.common.feature_flags import (
    eval_feature_flag_statuses,
    FeatureFlagSet,
)

INFRA_DIR = Path(__file__).parent
TEMPLATES = {
    "{{llm_app_name|lower|replace('-','_')}}__deployed_llm",
    "{{llm_app_name|lower|replace('-','_')}}__llm_blueprint_with_llm_gateway",
    "{{llm_app_name|lower|replace('-','_')}}__llm_blueprint_with_provided_llm",
    "{{llm_app_name|lower|replace('-','_')}}__llm_gateway_direct",
}


def get_litellm_provider_credentials(model_identifier: str) -> Dict[str, Any]:
    """
    Use LiteLLM to dynamically discover what credentials are needed for a model.

    This leverages LiteLLM's internal logic to determine required environment variables
    and credential patterns for any supported provider.
    """
    try:
        # First, let LiteLLM tell us what provider this is
        provider_info = get_llm_provider(model=model_identifier)
        provider_name = provider_info[0] if provider_info else None

        if not provider_name:
            raise ValueError(
                f"LiteLLM doesn't recognize provider for: {model_identifier}"
            )

        # Try to make a test call - this will tell us what environment variables are missing
        try:
            completion(
                model=model_identifier,
                messages=[{"role": "user", "content": "test"}],
                max_tokens=1,
            )
            # If it succeeds, extract the env vars that were actually used
            credentials = _extract_active_credentials(model_identifier)
            return {
                "provider": provider_name,
                "credentials": credentials,
                "valid": True,
            }

        except Exception as e:
            # Parse the error to understand what credentials are needed
            credentials = _parse_credential_requirements_from_error(
                str(e), provider_name
            )
            return {
                "provider": provider_name,
                "credentials": credentials,
                "valid": False,
            }

    except Exception as e:
        pulumi.warn(f"Failed to determine credentials for {model_identifier}: {str(e)}")
        return {"provider": "unknown", "credentials": {}, "valid": False}


def _extract_active_credentials(model_identifier: str) -> Dict[str, str]:
    """Extract environment variables that are currently being used by LiteLLM."""
    credentials = {}

    # Check what environment variables are set that LiteLLM would use
    env_vars_to_check = [
        "OPENAI_API_KEY",
        "OPENAI_API_BASE",
        "OPENAI_API_VERSION",
        "OPENAI_ORGANIZATION",
        "AZURE_API_KEY",
        "AZURE_API_BASE",
        "AZURE_API_VERSION",
        "ANTHROPIC_API_KEY",
        "COHERE_API_KEY",
        "HUGGINGFACE_API_KEY",
        "AWS_ACCESS_KEY_ID",
        "AWS_SECRET_ACCESS_KEY",
        "AWS_SESSION_TOKEN",
        "AWS_REGION",
        "GOOGLE_APPLICATION_CREDENTIALS",
        "VERTEX_PROJECT",
        "VERTEX_LOCATION",
    ]

    for env_var in env_vars_to_check:
        value = os.environ.get(env_var)
        if value:
            credentials[env_var] = value

    return credentials


def _parse_credential_requirements_from_error(
    error_message: str, provider: str
) -> Dict[str, Optional[str]]:
    """Parse LiteLLM error messages to understand what credentials are missing."""
    credentials = {}

    # LiteLLM error messages typically mention the missing environment variables
    error_lower = error_message.lower()

    # Common patterns in LiteLLM error messages
    if "openai_api_key" in error_lower or "api_key" in error_lower:
        if "azure" in provider.lower():
            credentials["OPENAI_API_KEY"] = os.environ.get(
                "AZURE_API_KEY"
            ) or os.environ.get("OPENAI_API_KEY")
            credentials["OPENAI_API_BASE"] = os.environ.get(
                "AZURE_API_BASE"
            ) or os.environ.get("OPENAI_API_BASE")
            credentials["OPENAI_API_VERSION"] = os.environ.get(
                "AZURE_API_VERSION"
            ) or os.environ.get("OPENAI_API_VERSION")
        else:
            credentials["OPENAI_API_KEY"] = os.environ.get("OPENAI_API_KEY")

    if "anthropic_api_key" in error_lower:
        credentials["ANTHROPIC_API_KEY"] = os.environ.get("ANTHROPIC_API_KEY")

    if "aws" in error_lower or "bedrock" in error_lower:
        credentials["AWS_ACCESS_KEY_ID"] = os.environ.get("AWS_ACCESS_KEY_ID")
        credentials["AWS_SECRET_ACCESS_KEY"] = os.environ.get("AWS_SECRET_ACCESS_KEY")
        credentials["AWS_REGION"] = os.environ.get("AWS_REGION")

    if "google" in error_lower or "vertex" in error_lower:
        credentials["GOOGLE_APPLICATION_CREDENTIALS"] = os.environ.get(
            "GOOGLE_APPLICATION_CREDENTIALS"
        )

    return credentials


def create_datarobot_credentials_from_litellm(
    model_identifier: str,
    app_name: str,
) -> List[pulumi_datarobot.CustomModelRuntimeParameterValueArgs]:
    """
    Create DataRobot credential objects from LiteLLM requirements.

    This is much cleaner - let LiteLLM figure out what's needed, then we just
    convert to DataRobot format.
    """
    provider_info = get_litellm_provider_credentials(model_identifier)
    credentials = provider_info["credentials"]

    if not credentials:
        pulumi.warn(f"No credentials found for model {model_identifier}")
        return []

    runtime_params = []
    stack_name = pulumi.get_stack()

    # Handle different credential types based on what LiteLLM needs
    for cred_key, cred_value in credentials.items():
        if not cred_value:
            continue

        param_key = cred_key

        # Create appropriate DataRobot credential type
        if cred_key in ["AWS_ACCESS_KEY_ID", "AWS_SECRET_ACCESS_KEY"]:
            # Group AWS credentials together
            if (
                cred_key == "AWS_ACCESS_KEY_ID"
                and "AWS_SECRET_ACCESS_KEY" in credentials
            ):
                aws_credential = pulumi_datarobot.AwsCredential(
                    resource_name=f"{app_name} AWS Credential [{stack_name}]",
                    aws_access_key_id=credentials["AWS_ACCESS_KEY_ID"],
                    aws_secret_access_key=credentials["AWS_SECRET_ACCESS_KEY"],
                    aws_session_token=credentials.get("AWS_SESSION_TOKEN"),
                )
                runtime_params.append(
                    pulumi_datarobot.CustomModelRuntimeParameterValueArgs(
                        key="AWS_ACCOUNT",
                        type="credential",
                        value=aws_credential.id,
                    )
                )
            # Skip individual AWS keys since we handle them as a group
            continue

        elif cred_key == "GOOGLE_APPLICATION_CREDENTIALS":
            # Handle Google credentials
            try:
                with open(cred_value, "r") as f:
                    service_account_key = json.load(f)

                google_credential = pulumi_datarobot.GoogleCloudCredential(
                    resource_name=f"{app_name} Google Credential [{stack_name}]",
                    gcp_key=json.dumps(service_account_key),
                )
                runtime_params.append(
                    pulumi_datarobot.CustomModelRuntimeParameterValueArgs(
                        key="GOOGLE_SERVICE_ACCOUNT",
                        type="credential",
                        value=google_credential.id,
                    )
                )
            except Exception as e:
                pulumi.warn(f"Failed to load Google credentials from {cred_value}: {e}")
            continue

        else:
            # Handle API key credentials
            if "key" in cred_key.lower() or "token" in cred_key.lower():
                api_credential = pulumi_datarobot.ApiTokenCredential(
                    resource_name=f"{app_name} {cred_key} [{stack_name}]",
                    api_token=cred_value,
                )
                runtime_params.append(
                    pulumi_datarobot.CustomModelRuntimeParameterValueArgs(
                        key=param_key,
                        type="credential",
                        value=api_credential.id,
                    )
                )
            else:
                # Handle string parameters (endpoints, versions, etc.)
                runtime_params.append(
                    pulumi_datarobot.CustomModelRuntimeParameterValueArgs(
                        key=param_key,
                        type="string",
                        value=cred_value,
                    )
                )

    return runtime_params


def require_single_selection():
    enabled_llms_count = 0
    for llm_module in INFRA_DIR.glob("{{llm_app_name|lower|replace('-','_')}}__*.py"):
        if llm_module.is_file():
            enabled_llms_count += 1
    if enabled_llms_count != 1:
        raise ValueError("Exactly one LLM module must be enabled for {{llm_app_name}}.")


def validate_feature_flags(flags: FeatureFlagSet) -> None:
    corrections, invalid_flags = eval_feature_flag_statuses(flags)
    for flag in invalid_flags:
        correct_value = flags[flag]
        pulumi.warn(
            f"Feature flag '{flag}' is required to be {correct_value} but is no longer a valid DataRobot feature flag."
        )
    if invalid_flags:
        raise ValueError(f"Invalid feature flags detected: {invalid_flags}")
    for flag, correct_value in corrections:
        pulumi.error(
            f"This app template requires that feature flag '{flag}' is set "
            f"to {correct_value}. Contact your DataRobot representative for "
            "assistance."
        )
    if corrections:
        raise pulumi.RunError("Please correct feature flag settings and run again.")


def verify_llm_gateway_model_availability(model_id: str) -> None:
    """
    Validate the model is in the catalog
    """
    dr_client = datarobot.Client()
    response = dr_client.get("genai/llmgw/catalog/")
    data = response.json()
    non_deprecated_models = "\n.   - ".join(
        [
            model["model"]
            for model in data["data"]
            if not model["isDeprecated"] and model["isActive"]
        ]
    )
    matched_models = [
        model
        for model in data["data"]
        if (model["model"] == model_id or model["llmId"] == model_id)
    ]
    if not matched_models:
        raise ValueError(
            f"Model '{model_id}' not found in catalog. Available models: {non_deprecated_models}"
        )
    if not len(matched_models) == 1:
        raise ValueError(
            f"Multiple models found for '{model_id}' in catalog. {matched_models}"
        )
    if not matched_models[0]["isActive"] or matched_models[0]["isDeprecated"]:
        raise ValueError(
            f"Model '{model_id}' is not active or is deprecated. Available models: {non_deprecated_models}"
        )


def verify_llm(model_id: str | None, deployment_id: str | None=None) -> None:
    """
    Verify that the specified LLM is valid, available, and you can say hello
    """
    # Pre-existing deployment
    if deployment_id:
        dr_client = datarobot.Client()
        deployment_chat_base_url = (
            f"{dr_client.endpoint.rstrip('/')}/deployments/{deployment_id}/"
        )
        completion(
            model="datarobot/datarobot-deployed-llm",
            messages=[{"content": "Hi", "role": "user"}],
            api_base=deployment_chat_base_url,
        )
        return

    completion(
        model=f"{model_id}",
        messages=[{"content": "Hi", "role": "user"}],
    )


require_single_selection()
